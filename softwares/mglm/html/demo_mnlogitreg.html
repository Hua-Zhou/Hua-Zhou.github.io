
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Multinomial-logit regression and sparse regression</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-07-09"><meta name="DC.source" content="demo_mnlogitreg.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Multinomial-logit regression and sparse regression</h1><!--introduction--><p>A demo of Multinomial-logit regression and sparse regression</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Generate multinomial random vectors from covariates</a></li><li><a href="#2">Fit multinomial logit regression</a></li><li><a href="#3">Fit multinomial logit sparse regression - - lasso/group/nuclear penalty</a></li></ul></div><h2>Generate multinomial random vectors from covariates<a name="1"></a></h2><pre class="codeinput">clear;
<span class="comment">% reset random seed</span>
s = RandStream(<span class="string">'mt19937ar'</span>,<span class="string">'Seed'</span>,1);
RandStream.setGlobalStream(s);
<span class="comment">% sample size</span>
n = 200;
<span class="comment">% # covariates</span>
p = 15;
<span class="comment">% # bins</span>
d = 5;
<span class="comment">% design matrix</span>
X = randn(n,p);
<span class="comment">% true regression coefficients: predictors 1, 3, and 5 have effects</span>
B = zeros(p,d-1);
nzidx = [1 3 5];
B(nzidx,:) = ones(length(nzidx),d-1);
prob = zeros(n,d);
prob(:,d) = ones(n,1);
prob(:,1:d-1) = exp(X*B);
prob = bsxfun(@times, prob, 1./sum(prob,2));
batchsize = 25+unidrnd(25,n,1);
Y = mnrnd(batchsize,prob);

zerorows = sum(Y,2);
Y=Y(zerorows~=0, :);
X=X(zerorows~=0, :);
</pre><h2>Fit multinomial logit regression<a name="2"></a></h2><pre class="codeinput">tic;
[B_hat, stats] = mnlogitreg(X,Y);
toc;
display(B_hat);
display(stats.se);
display(stats);
<span class="comment">% Wald test of predictor significance</span>
display(<span class="string">'Wald test p-values:'</span>);
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel(<span class="string">'iteration'</span>);
ylabel(<span class="string">'log-likelihood'</span>);
</pre><pre class="codeoutput">Elapsed time is 0.065748 seconds.

B_hat =

    0.9062    0.9845    0.9823    1.0066
    0.0491    0.0174    0.0223   -0.0064
    0.9543    0.9696    0.9683    0.9576
   -0.0121   -0.0053    0.0275    0.0129
    0.9871    1.0022    0.9483    0.9692
    0.0010   -0.0019   -0.0619    0.0244
   -0.0288   -0.0399   -0.0092   -0.0297
   -0.0591    0.0135   -0.0068    0.0009
   -0.0469   -0.0452   -0.0606   -0.0716
    0.0122   -0.0213   -0.0476   -0.0115
    0.0758   -0.0163    0.0003    0.0828
   -0.0039   -0.0010   -0.0128   -0.0097
   -0.0780   -0.0469   -0.0415   -0.0474
    0.0318   -0.0152   -0.0228    0.0117
   -0.0009   -0.0230   -0.0645   -0.0423


ans =

    0.0430    0.0434    0.0434    0.0436
    0.0454    0.0456    0.0455    0.0455
    0.0420    0.0421    0.0421    0.0423
    0.0415    0.0416    0.0414    0.0417
    0.0441    0.0442    0.0438    0.0441
    0.0381    0.0380    0.0380    0.0380
    0.0380    0.0382    0.0380    0.0382
    0.0387    0.0385    0.0385    0.0388
    0.0378    0.0379    0.0378    0.0380
    0.0397    0.0400    0.0401    0.0399
    0.0433    0.0433    0.0430    0.0432
    0.0427    0.0427    0.0426    0.0427
    0.0411    0.0412    0.0412    0.0413
    0.0387    0.0387    0.0387    0.0388
    0.0398    0.0398    0.0398    0.0400


stats = 

                     BIC: 3.5804e+03
                     AIC: 3.3825e+03
                     dof: 60
              iterations: 5
                    logL: -1.6313e+03
               logL_iter: [1x5 double]
                    prob: [200x5 double]
                    yhat: [200x5 double]
                gradient: [60x1 double]
    observed_information: [60x60 double]
                       H: [60x60 double]
                      se: [15x4 double]
               wald_stat: [1x15 double]
             wald_pvalue: [1x15 double]

Wald test p-values:

ans =

  Columns 1 through 7

         0    0.7611         0    0.8724         0    0.2120    0.8340

  Columns 8 through 14

    0.3342    0.3851    0.6591    0.0494    0.9974    0.4458    0.6268

  Column 15

    0.4288

</pre><img vspace="5" hspace="5" src="demo_mnlogitreg_01.png" alt=""> <h2>Fit multinomial logit sparse regression - - lasso/group/nuclear penalty<a name="3"></a></h2><pre class="codeinput">penalty = {<span class="string">'sweep'</span>,<span class="string">'group'</span>,<span class="string">'nuclear'</span>};
ngridpt = 10;
dist = <span class="string">'mnlogit'</span>;

<span class="keyword">for</span> i = 1:length(penalty)

    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),-log(size(X,1)),ngridpt));
    BICs = zeros(1,ngridpt);
    LogLs = zeros(1,ngridpt);
    Dofs =zeros(1, ngridpt);
    tic;
    <span class="keyword">for</span> j=1:ngridpt
        <span class="keyword">if</span> j==1
            B0 = zeros(p,d-1);
        <span class="keyword">else</span>
            B0 = B_hat;
        <span class="keyword">end</span>
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),<span class="string">'penalty'</span>,pen, <span class="keyword">...</span>
            <span class="string">'dist'</span>,dist,<span class="string">'B0'</span>,B0);
        BICs(j) = stats.BIC;
        LogLs(j) = stats.logL;
        Dofs(j) = stats.dof;
    <span class="keyword">end</span>
    toc;

    <span class="comment">% True signal versus estimated signal</span>
    [bestbic,bestidx] = min(BICs);
    B_best = mglm_sparsereg(X,Y,lambdas(bestidx),<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    <span class="comment">% display MSE of regularized estiamte</span>
    display(norm(B_best-B,2)/sqrt(numel(B)));
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel(<span class="string">'BIC'</span>);
    xlabel(<span class="string">'\lambda'</span>);
    xlim([min(lambdas) max(lambdas)]);
    subplot(1,3,2);
    imshow(mat2gray(-B)); title(<span class="string">'True B'</span>);
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen <span class="string">' estimate'</span>]);

<span class="keyword">end</span>
</pre><pre class="codeoutput">Elapsed time is 0.298990 seconds.

ans =

    0.0751

Elapsed time is 0.168264 seconds.

ans =

    0.0638

Elapsed time is 0.225665 seconds.

ans =

    0.1726

</pre><img vspace="5" hspace="5" src="demo_mnlogitreg_02.png" alt=""> <img vspace="5" hspace="5" src="demo_mnlogitreg_03.png" alt=""> <img vspace="5" hspace="5" src="demo_mnlogitreg_04.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Multinomial-logit regression and sparse regression
% A demo of Multinomial-logit regression and sparse regression

%% Generate multinomial random vectors from covariates

clear;
% reset random seed
s = RandStream('mt19937ar','Seed',1);
RandStream.setGlobalStream(s);
% sample size
n = 200;
% # covariates
p = 15;
% # bins
d = 5;
% design matrix
X = randn(n,p); 
% true regression coefficients: predictors 1, 3, and 5 have effects
B = zeros(p,d-1);
nzidx = [1 3 5];
B(nzidx,:) = ones(length(nzidx),d-1);
prob = zeros(n,d);
prob(:,d) = ones(n,1);
prob(:,1:d-1) = exp(X*B);
prob = bsxfun(@times, prob, 1./sum(prob,2));
batchsize = 25+unidrnd(25,n,1);
Y = mnrnd(batchsize,prob);

zerorows = sum(Y,2);
Y=Y(zerorows~=0, :);
X=X(zerorows~=0, :);

%% Fit multinomial logit regression

tic;
[B_hat, stats] = mnlogitreg(X,Y);
toc;
display(B_hat);
display(stats.se);
display(stats);
% Wald test of predictor significance
display('Wald test p-values:');
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel('iteration');
ylabel('log-likelihood');

%% Fit multinomial logit sparse regression - - lasso/group/nuclear penalty

penalty = {'sweep','group','nuclear'};
ngridpt = 10;
dist = 'mnlogit';

for i = 1:length(penalty)
    
    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,'penalty',pen,'dist',dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),-log(size(X,1)),ngridpt));
    BICs = zeros(1,ngridpt);
    LogLs = zeros(1,ngridpt);
    Dofs =zeros(1, ngridpt);
    tic;
    for j=1:ngridpt
        if j==1
            B0 = zeros(p,d-1);
        else
            B0 = B_hat;
        end
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),'penalty',pen, ...
            'dist',dist,'B0',B0);
        BICs(j) = stats.BIC;
        LogLs(j) = stats.logL;
        Dofs(j) = stats.dof;
    end
    toc;
    
    % True signal versus estimated signal
    [bestbic,bestidx] = min(BICs);
    B_best = mglm_sparsereg(X,Y,lambdas(bestidx),'penalty',pen,'dist',dist);
    % display MSE of regularized estiamte
    display(norm(B_best-B,2)/sqrt(numel(B)));
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel('BIC');
    xlabel('\lambda');
    xlim([min(lambdas) max(lambdas)]);    
    subplot(1,3,2);
    imshow(mat2gray(-B)); title('True B');
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen ' estimate']);

end
##### SOURCE END #####
--></body></html>