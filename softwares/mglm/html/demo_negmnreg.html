
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Negative multinomial regression and sparse regression</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-07-09"><meta name="DC.source" content="demo_negmnreg.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Negative multinomial regression and sparse regression</h1><!--introduction--><p>A demo of negative multinomial regression and sparse regression</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Generate negative multinomial random vectors from covariates</a></li><li><a href="#2">Fit negative multinomial regression - link over-disperson parameter</a></li><li><a href="#3">Fit negative multinomial regression - not linking over-disperson parameter</a></li><li><a href="#4">Fit negative multinomial sparse regression - lasso/group/nuclear penalty</a></li><li><a href="#5">Sparse regression (not linking over-disp.) - lasso/group/nuclear penalty</a></li></ul></div><h2>Generate negative multinomial random vectors from covariates<a name="1"></a></h2><pre class="codeinput">clear;
<span class="comment">% reset random seed</span>
s = RandStream(<span class="string">'mt19937ar'</span>,<span class="string">'Seed'</span>,1);
RandStream.setGlobalStream(s);
<span class="comment">% sample size</span>
n = 200;
<span class="comment">% # covariates</span>
p = 15;
<span class="comment">% # bins</span>
d = 5;
<span class="comment">% design matrix</span>
X = randn(n,p);
<span class="comment">% true regression coefficients</span>
B = zeros(p,d+1);
nzidx = [1 3 5];
B(nzidx,:) = ones(length(nzidx),d+1);
eta = X*B;
alpha = exp(eta);
prob(:,d+1) = 1./(sum(alpha(:,1:d),2)+1);
prob(:,1:d) = bsxfun(@times, alpha(:,1:d), prob(:,d+1));
b= binornd(10,0.2, n, 1);
Y = negmnrnd(prob,b);

zerorows = sum(Y,2);
Y=Y(zerorows~=0, :);
X=X(zerorows~=0, :);
</pre><h2>Fit negative multinomial regression - link over-disperson parameter<a name="2"></a></h2><pre class="codeinput">tic;
[B_hat, stats] = negmnreg(X,Y);
toc;
display(B_hat);
display(stats);
<span class="comment">% Wald test of predictor significance</span>
display(<span class="string">'Wald test p-values:'</span>);
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel(<span class="string">'iteration'</span>);
ylabel(<span class="string">'log-likelihood'</span>);
</pre><pre class="codeoutput">Elapsed time is 0.280617 seconds.

B_hat =

    1.0559    1.0459    1.0098    1.0084    0.9932   -0.1119
    0.0278   -0.0052    0.0070    0.0158    0.0092    0.1827
    0.8343    0.8947    0.9116    0.8642    0.8333    0.2859
    0.0411   -0.0185    0.0076   -0.0617   -0.0201    0.0264
    0.9650    1.0042    1.0083    1.0122    1.0710   -0.1664
   -0.1669   -0.1084   -0.0812   -0.1458   -0.0916    0.0577
   -0.1095   -0.0959   -0.0644   -0.1221   -0.0612    0.3709
    0.1659    0.1893    0.1286    0.1494    0.0899   -0.2356
   -0.0414    0.0081   -0.1278   -0.0591   -0.0835    0.2748
   -0.0397   -0.1438   -0.0578   -0.0818   -0.1170   -0.0070
    0.2200    0.3404    0.2959    0.2698    0.2547   -0.5784
    0.1163    0.1595    0.1778    0.0901    0.1181   -0.1753
   -0.1064   -0.0661   -0.1113   -0.0682   -0.0980    0.0240
    0.0613    0.0525    0.0720    0.0194    0.0251   -0.0206
   -0.0697   -0.0377    0.0490    0.0055   -0.0188    0.1182


stats = 

                     BIC: 3.5447e+03
                     dof: 90
              iterations: 15
                    logL: -1.5451e+03
               logL_iter: [1x15 double]
                      se: [15x6 double]
               wald_stat: [1x15 double]
             wald_pvalue: [1x15 double]
                       H: [90x90 double]
                gradient: [90x1 double]
    observed_information: [90x90 double]

Wald test p-values:

ans =

  Columns 1 through 7

         0    0.6526         0    0.6493         0    0.4724    0.0606

  Columns 8 through 14

    0.2285    0.0125    0.4257    0.0038    0.4790    0.8500    0.9286

  Column 15

    0.3161

</pre><img vspace="5" hspace="5" src="demo_negmnreg_01.png" alt=""> <h2>Fit negative multinomial regression - not linking over-disperson parameter<a name="3"></a></h2><pre class="codeinput">tic;
[B_hat, b_hat, stats] = negmnreg2(X,Y);
toc;
disp(B_hat);
disp(stats.se_B);
disp(b_hat);
disp(stats.se_b);
display(stats);
<span class="comment">% Wald test of predictor significance</span>
display(<span class="string">'Wald test p-values:'</span>);
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel(<span class="string">'iteration'</span>);
ylabel(<span class="string">'log-likelihood'</span>);
</pre><pre class="codeoutput">Elapsed time is 0.160916 seconds.
    0.8703    0.8610    0.8284    0.8264    0.8117
    0.1526    0.1215    0.1320    0.1414    0.1345
    0.9052    0.9645    0.9824    0.9348    0.9053
    0.0244   -0.0311   -0.0085   -0.0746   -0.0338
    0.7500    0.7874    0.7897    0.7939    0.8518
   -0.1307   -0.0742   -0.0470   -0.1106   -0.0575
    0.0262    0.0408    0.0669    0.0126    0.0717
    0.0421    0.0648    0.0063    0.0266   -0.0318
    0.0491    0.0947   -0.0345    0.0310    0.0079
   -0.0425   -0.1418   -0.0618   -0.0832   -0.1186
   -0.0724    0.0411    0.0022   -0.0252   -0.0394
    0.0828    0.1263    0.1437    0.0583    0.0849
   -0.1464   -0.1080   -0.1518   -0.1103   -0.1390
    0.0242    0.0186    0.0351   -0.0156   -0.0100
   -0.0008    0.0313    0.1110    0.0707    0.0458

    0.0760    0.0756    0.0756    0.0758    0.0758
    0.0757    0.0754    0.0755    0.0758    0.0757
    0.0672    0.0667    0.0666    0.0671    0.0671
    0.0768    0.0766    0.0765    0.0767    0.0766
    0.0765    0.0764    0.0763    0.0766    0.0763
    0.0660    0.0655    0.0660    0.0662    0.0660
    0.0699    0.0694    0.0694    0.0698    0.0697
    0.0617    0.0612    0.0614    0.0618    0.0614
    0.0712    0.0708    0.0709    0.0712    0.0709
    0.0745    0.0739    0.0738    0.0744    0.0742
    0.0752    0.0753    0.0750    0.0754    0.0755
    0.0679    0.0674    0.0673    0.0673    0.0674
    0.0722    0.0718    0.0720    0.0722    0.0722
    0.0671    0.0666    0.0670    0.0670    0.0670
    0.0692    0.0690    0.0687    0.0691    0.0688

    2.2915

    0.1355


stats = 

                     BIC: 3.3630e+03
              iterations: 8
                    logL: -1.4921e+03
               logL_iter: [1x8 double]
                      se: [15x6 double]
               wald_stat: [1x15 double]
             wald_pvalue: [1x15 double]
                       H: [76x76 double]
                gradient: [76x1 double]
    observed_information: [76x76 double]
                    se_B: [15x5 double]
                    se_b: 0.1355

Wald test p-values:

ans =

  Columns 1 through 7

         0    0.4959         0    0.5382         0    0.3026    0.7689

  Columns 8 through 14

    0.2618    0.1964    0.2896    0.4888    0.1913    0.3636    0.8945

  Column 15

    0.3568

</pre><img vspace="5" hspace="5" src="demo_negmnreg_02.png" alt=""> <h2>Fit negative multinomial sparse regression - lasso/group/nuclear penalty<a name="4"></a></h2><p>Regression on the over dispersion parameter</p><pre class="codeinput">penalty = {<span class="string">'sweep'</span>,<span class="string">'group'</span>,<span class="string">'nuclear'</span>};
ngridpt = 30;
dist = <span class="string">'negmn'</span>;

<span class="keyword">for</span> i = 1:length(penalty)

    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),log(maxlambda/100),ngridpt));
    BICs = zeros(1,ngridpt);
    logl =zeros(1, ngridpt);
    dofs = zeros(1, ngridpt);
    tic;
    <span class="keyword">for</span> j=1:ngridpt
        <span class="keyword">if</span> j==1
            B0 = zeros(p,d+1);
        <span class="keyword">else</span>
            B0 = B_hat;
        <span class="keyword">end</span>
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),<span class="string">'penalty'</span>,pen, <span class="keyword">...</span>
            <span class="string">'dist'</span>,dist,<span class="string">'B0'</span>,B0);
        BICs(j) = stats.BIC;
        logl(j) = stats.logL;
        dofs(j) = stats.dof;
    <span class="keyword">end</span>
    toc;

    <span class="comment">% True signal versus estimated signal</span>
    [bestbic,bestidx] = min(BICs);
    [B_best,stats] = mglm_sparsereg(X,Y,lambdas(bestidx),<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel(<span class="string">'BIC'</span>);
    xlabel(<span class="string">'\lambda'</span>);
    xlim([min(lambdas) max(lambdas)]);
    subplot(1,3,2);
    imshow(mat2gray(-B)); title(<span class="string">'True B'</span>);
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen <span class="string">' estimate'</span>]);

<span class="keyword">end</span>
</pre><pre class="codeoutput">Elapsed time is 3.496622 seconds.
Elapsed time is 3.566114 seconds.
Elapsed time is 5.404825 seconds.
</pre><img vspace="5" hspace="5" src="demo_negmnreg_03.png" alt=""> <img vspace="5" hspace="5" src="demo_negmnreg_04.png" alt=""> <img vspace="5" hspace="5" src="demo_negmnreg_05.png" alt=""> <h2>Sparse regression (not linking over-disp.) - lasso/group/nuclear penalty<a name="5"></a></h2><p>Do not run regression on the over dispersion parameter</p><pre class="codeinput">penalty = {<span class="string">'sweep'</span>,<span class="string">'group'</span>,<span class="string">'nuclear'</span>};
ngridpt = 30;
dist = <span class="string">'negmn2'</span>;

<span class="keyword">for</span> i = 1:length(penalty)

    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),log(maxlambda/100),ngridpt));
    BICs = zeros(1,ngridpt);
    logl =zeros(1, ngridpt);
    dofs = zeros(1, ngridpt);
    tic;
    <span class="keyword">for</span> j=1:ngridpt
        <span class="keyword">if</span> j==1
            B0 = zeros(p,d);
        <span class="keyword">else</span>
            B0 = B_hat;
        <span class="keyword">end</span>
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),<span class="string">'penalty'</span>,pen, <span class="keyword">...</span>
            <span class="string">'dist'</span>,dist,<span class="string">'B0'</span>,B0);
        BICs(j) = stats.BIC;
        logl(j) = stats.logL;
        dofs(j) = stats.dof;
    <span class="keyword">end</span>
    toc;

    <span class="comment">% True signal versus estimated signal</span>
    [bestbic,bestidx] = min(BICs);
    B_best = mglm_sparsereg(X,Y,lambdas(bestidx),<span class="string">'penalty'</span>,pen,<span class="string">'dist'</span>,dist);
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel(<span class="string">'BIC'</span>);
    xlabel(<span class="string">'\lambda'</span>);
    xlim([min(lambdas) max(lambdas)]);
    subplot(1,3,2);
    imshow(mat2gray(-B)); title(<span class="string">'True B'</span>);
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen <span class="string">' estimate'</span>]);

<span class="keyword">end</span>
</pre><pre class="codeoutput">Elapsed time is 9.926697 seconds.
Elapsed time is 9.559092 seconds.
Elapsed time is 10.367473 seconds.
</pre><img vspace="5" hspace="5" src="demo_negmnreg_06.png" alt=""> <img vspace="5" hspace="5" src="demo_negmnreg_07.png" alt=""> <img vspace="5" hspace="5" src="demo_negmnreg_08.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Negative multinomial regression and sparse regression
% A demo of negative multinomial regression and sparse regression

%% Generate negative multinomial random vectors from covariates

clear;
% reset random seed
s = RandStream('mt19937ar','Seed',1);
RandStream.setGlobalStream(s);
% sample size
n = 200;
% # covariates
p = 15;
% # bins
d = 5;      
% design matrix
X = randn(n,p); 
% true regression coefficients
B = zeros(p,d+1);  
nzidx = [1 3 5];
B(nzidx,:) = ones(length(nzidx),d+1);
eta = X*B;
alpha = exp(eta);
prob(:,d+1) = 1./(sum(alpha(:,1:d),2)+1);
prob(:,1:d) = bsxfun(@times, alpha(:,1:d), prob(:,d+1));
b= binornd(10,0.2, n, 1);
Y = negmnrnd(prob,b);

zerorows = sum(Y,2);
Y=Y(zerorows~=0, :);
X=X(zerorows~=0, :);

%% Fit negative multinomial regression - link over-disperson parameter

tic;
[B_hat, stats] = negmnreg(X,Y);
toc;
display(B_hat);
display(stats);
% Wald test of predictor significance
display('Wald test p-values:');
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel('iteration');
ylabel('log-likelihood');

%% Fit negative multinomial regression - not linking over-disperson parameter

tic;
[B_hat, b_hat, stats] = negmnreg2(X,Y);
toc;
disp(B_hat);
disp(stats.se_B);
disp(b_hat);
disp(stats.se_b);
display(stats);
% Wald test of predictor significance
display('Wald test p-values:');
display(stats.wald_pvalue);

figure;
plot(stats.logL_iter);
xlabel('iteration');
ylabel('log-likelihood');

%% Fit negative multinomial sparse regression - lasso/group/nuclear penalty
% Regression on the over dispersion parameter

penalty = {'sweep','group','nuclear'};
ngridpt = 30;
dist = 'negmn';

for i = 1:length(penalty)
    
    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,'penalty',pen,'dist',dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),log(maxlambda/100),ngridpt));
    BICs = zeros(1,ngridpt);
    logl =zeros(1, ngridpt);
    dofs = zeros(1, ngridpt);
    tic;
    for j=1:ngridpt
        if j==1
            B0 = zeros(p,d+1);
        else
            B0 = B_hat;
        end
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),'penalty',pen, ...
            'dist',dist,'B0',B0);
        BICs(j) = stats.BIC;
        logl(j) = stats.logL;
        dofs(j) = stats.dof;
    end
    toc;
    
    % True signal versus estimated signal
    [bestbic,bestidx] = min(BICs);
    [B_best,stats] = mglm_sparsereg(X,Y,lambdas(bestidx),'penalty',pen,'dist',dist);
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel('BIC');
    xlabel('\lambda');
    xlim([min(lambdas) max(lambdas)]);    
    subplot(1,3,2);
    imshow(mat2gray(-B)); title('True B');
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen ' estimate']);

end

%% Sparse regression (not linking over-disp.) - lasso/group/nuclear penalty
% Do not run regression on the over dispersion parameter

penalty = {'sweep','group','nuclear'};
ngridpt = 30;
dist = 'negmn2';

for i = 1:length(penalty)
    
    pen = penalty{i};
    [~, stats] = mglm_sparsereg(X,Y,inf,'penalty',pen,'dist',dist);
    maxlambda = stats.maxlambda;

    lambdas = exp(linspace(log(maxlambda),log(maxlambda/100),ngridpt));
    BICs = zeros(1,ngridpt);
    logl =zeros(1, ngridpt);
    dofs = zeros(1, ngridpt);
    tic;
    for j=1:ngridpt
        if j==1
            B0 = zeros(p,d);
        else
            B0 = B_hat;
        end
        [B_hat, stats] = mglm_sparsereg(X,Y,lambdas(j),'penalty',pen, ...
            'dist',dist,'B0',B0);
        BICs(j) = stats.BIC;
        logl(j) = stats.logL;
        dofs(j) = stats.dof;
    end
    toc;
    
    % True signal versus estimated signal
    [bestbic,bestidx] = min(BICs);
    B_best = mglm_sparsereg(X,Y,lambdas(bestidx),'penalty',pen,'dist',dist);
    figure;
    subplot(1,3,1);
    semilogx(lambdas,BICs);
    ylabel('BIC');
    xlabel('\lambda');
    xlim([min(lambdas) max(lambdas)]);    
    subplot(1,3,2);
    imshow(mat2gray(-B)); title('True B');
    subplot(1,3,3);
    imshow(mat2gray(-B_best)); title([pen ' estimate']);

end

##### SOURCE END #####
--></body></html>