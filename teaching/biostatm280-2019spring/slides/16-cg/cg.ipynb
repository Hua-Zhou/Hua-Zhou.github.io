{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Conjugate-Gradient-and-Krylov-Subspace-Methods\" data-toc-modified-id=\"Conjugate-Gradient-and-Krylov-Subspace-Methods-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Conjugate Gradient and Krylov Subspace Methods</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Conjugate-gradient-(CG)-method\" data-toc-modified-id=\"Conjugate-gradient-(CG)-method-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Conjugate gradient (CG) method</a></div><div class=\"lev2 toc-item\"><a href=\"#Pre-conditioned-conjugate-gradient-(PCG)\" data-toc-modified-id=\"Pre-conditioned-conjugate-gradient-(PCG)-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pre-conditioned conjugate gradient (PCG)</a></div><div class=\"lev3 toc-item\"><a href=\"#Example-of-PCG\" data-toc-modified-id=\"Example-of-PCG-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Example of PCG</a></div><div class=\"lev2 toc-item\"><a href=\"#Other-Krylov-subspace-methods\" data-toc-modified-id=\"Other-Krylov-subspace-methods-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Other Krylov subspace methods</a></div><div class=\"lev2 toc-item\"><a href=\"#Software\" data-toc-modified-id=\"Software-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Software</a></div><div class=\"lev3 toc-item\"><a href=\"#Matlab\" data-toc-modified-id=\"Matlab-151\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Matlab</a></div><div class=\"lev3 toc-item\"><a href=\"#Julia\" data-toc-modified-id=\"Julia-152\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Julia</a></div><div class=\"lev4 toc-item\"><a href=\"#Least-squares-example\" data-toc-modified-id=\"Least-squares-example-1521\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>Least squares example</a></div><div class=\"lev2 toc-item\"><a href=\"#Further-reading\" data-toc-modified-id=\"Further-reading-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Further reading</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.1.0\n",
      "Commit 80516ca202 (2019-01-21 21:24 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.1 (ORCJIT, skylake)\n",
      "Environment:\n",
      "  JULIA_EDITOR = code\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Krylov Subspace Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* Conjugate gradient is the top-notch iterative method for solving large, **structured** linear systems $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A}$ is pd.  \n",
    "Earlier we talked about Jacobi, Gauss-Seidel, and successive over-relaxation (SOR) as the classical iterative solvers. They are rarely used in practice due to slow convergence.  \n",
    "\n",
    "    [Kershaw's results](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub) for a fusion problem.\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky **conjugate gradient** | 25                   |\n",
    "\n",
    "\n",
    "* History: Hestenes (**UCLA** professor!) and Stiefel proposed conjugate gradient method in 1950s.\n",
    "\n",
    "Hestenes and Stiefel (1952), [Methods of conjugate gradients for solving linear systems](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf), _Jounral of Research of the National Bureau of Standards_.\n",
    "\n",
    "* Solve linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **pd**, is equivalent to \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\text{minimize} \\,\\, f(\\mathbf{x}) = \\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Denote $\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b} =: r(\\mathbf{x})$.\n",
    "\n",
    "## Conjugate gradient (CG) method\n",
    "\n",
    "* Consider a simple idea: coordinate descent, that is to update components $x_j$ alternatingly. Same as the Gauss-Seidel iteration. Usually it takes too many iterations.\n",
    "\n",
    "<img src=\"coordinate_descent.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* A set of vectors $\\{\\mathbf{x}^{(0)},\\ldots,\\mathbf{x}^{(l)}\\}$ is said to be **conjugate with respect to $\\mathbf{A}$** if\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}_i^T \\mathbf{A} \\mathbf{p}_j = 0, \\quad \\text{for all } i \\ne j.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "For example, eigen-vectors of $\\mathbf{A}$ are conjugate to each other. Why?\n",
    "\n",
    "* **Conjugate direction** method: Given a set of conjugate vectors $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(l)}\\}$, at iteration $t$, we search along the conjugate direction $\\mathbf{p}^{(t)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\alpha^{(t)} = - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "is the optimal step length.\n",
    "\n",
    "* Theorem: In conjugate direction method, $\\mathbf{x}^{(t)}$ converges to the solution in **at most** $n$ steps.\n",
    "\n",
    "    Intuition: Look at graph.\n",
    "    \n",
    "<img src=\"conjugate_direction.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* **Conjugate gradient** method. Idea: generate $\\mathbf{p}^{(t)}$ using only $\\mathbf{p}^{(t-1)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}^{(t)} = - \\mathbf{r}^{(t)} + \\beta^{(t)} \\mathbf{p}^{(t-1)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\beta^{(t)}$ is determined by the conjugacy condition $\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t)} = 0$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\beta^{(t)} = \\frac{\\mathbf{r}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}{\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **CG algorithm (preliminary version)**:  \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{A} \\mathbf{x}^{(t+1)} - \\mathbf{b}$\n",
    "        4. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{A} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        5. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        6. $t \\gets t+1$\n",
    "        \n",
    "    Remark: The initial conjugate direction $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$ is crucial.\n",
    "        \n",
    "* Theorem: With CG algorithm\n",
    "    0. $\\mathbf{r}^{(t)}$ are mutually orthogonal. \n",
    "    0. $\\{\\mathbf{r}^{(0)},\\ldots,\\mathbf{r}^{(t)}\\}$ is contained in the **Krylov subspace** of degree $t$ for $\\mathbf{r}^{(0)}$, denoted by\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        {\\cal K}(\\mathbf{r}^{(0)}; t) = \\text{span} \\{\\mathbf{r}^{(0)},\\mathbf{A} \\mathbf{r}^{(0)}, \\mathbf{A}^2 \\mathbf{r}^{(0)}, \\ldots, \\mathbf{A}^{t} \\mathbf{r}^{(0)}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    0. $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(t)}\\}$ is contained in ${\\cal K}(\\mathbf{r}^{(0)}; t)$. \n",
    "    0. $\\mathbf{p}^{(0)}, \\ldots, \\mathbf{p}^{(t)}$ are conjugate with respect to $\\mathbf{A}$.  \n",
    "The iterates $\\mathbf{x}^{(t)}$ converge to the solution in at most $n$ steps.\n",
    "\n",
    "* **CG algorithm (economical version)**: saves one matrix-vector multiplication.\n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        4. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{r}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        5. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        6. $t \\gets t+1$\n",
    "\n",
    "* Computation cost per iteration is **one** matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t)}$.  \n",
    "Consider PageRank problem, $\\mathbf{A}$ has dimension $n \\approx 10^{10}$ but is highly structured (sparse + low rank). Each matrix vector multiplication takes $O(n)$.\n",
    "    \n",
    "* Theorem: If $\\mathbf{A}$ has $r$ distinct eigenvalues, $\\mathbf{x}^{(t)}$ converges to solution $\\mathbf{x}^*$ in at most $r$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-conditioned conjugate gradient (PCG)\n",
    "\n",
    "* Summary of conjugate gradient method for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ or equivalently minimizing $\\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} -  \\mathbf{b}^T \\mathbf{x}$:\n",
    "    * Each iteration needs one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t+1)}$. For structured $\\mathbf{A}$, often $O(n)$ cost per iteration.\n",
    "    * Guaranteed to converge in $n$ steps.\n",
    "    \n",
    "* Two important bounds for conjugate gradient algorithm:\n",
    "\n",
    "    Let $\\lambda_1 \\le \\cdots \\le \\lambda_n$ be the ordered eigenvalues of a pd $\\mathbf{A}$.  \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& \\left( \\frac{\\lambda_{n-t} - \\lambda_1}{\\lambda_{n-t} + \\lambda_1} \\right)^2 \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 \\\\\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& 2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})}-1}{\\sqrt{\\kappa(\\mathbf{A})}+1} \\right)^{t} \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2,\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\kappa(\\mathbf{A}) = \\lambda_n/\\lambda_1$ is the condition number of $\\mathbf{A}$.\n",
    "\n",
    "<img src=\"cg_twocluster_spectrum.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src=\"cg_twocluster_iterates.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* Messages:\n",
    "    * Roughly speaking, if the eigenvalues of $\\mathbf{A}$ occur in $r$ distinct clusters, the CG iterates will _approximately_ solve the problem after $O(r)$ steps.  \n",
    "    * $\\mathbf{A}$ with a small condition number ($\\lambda_1 \\approx \\lambda_n$) converges fast.\n",
    "    \n",
    "* **Pre-conditioning**: Change of variables $\\widehat{\\mathbf{x}} = \\mathbf{C} \\mathbf{x}$ via a nonsingular $\\mathbf{C}$ and solve\n",
    "$$\n",
    "\t(\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}) \\widehat{\\mathbf{x}} = \\mathbf{C}^{-T} \\mathbf{b}.\n",
    "$$\n",
    "Choose $\\mathbf{C}$ such that \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has small condition number, or \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has clustered eigenvalues\n",
    "    * Inexpensive solution of $\\mathbf{C}^T \\mathbf{C} \\mathbf{y} = \\mathbf{r}$\n",
    "    \n",
    "* Preconditioned CG does not make use of $\\mathbf{C}$ explicitly, but rather the matrix $\\mathbf{M} = \\mathbf{C}^T \\mathbf{C}$.\n",
    "\n",
    "\n",
    "* **Preconditioned CG (PCG)** algorithm: \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$, pre-conditioner $\\mathbf{M}$\n",
    "    0. $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$\n",
    "    0. solve $\\mathbf{M} \\mathbf{y}^{(0)} = \\mathbf{r}^{(0)}$ for $\\mathbf{y}^{(0)}$\n",
    "    0. $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        1. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{y}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        2. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        3. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        4. Solve $\\mathbf{M} \\mathbf{y}^{(t+1)} \\gets \\mathbf{r}^{(t+1)}$ for $\\mathbf{y}^{(t+1)}$\n",
    "        5. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{y}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        6. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{y}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        7. $t \\gets t+1$\n",
    "\n",
    "    Remark: Only extra cost in the pre-conditioned CG algorithm is the need to solve the linear system $\\mathbf{M} \\mathbf{y} = \\mathbf{r}$.\n",
    "    \n",
    "* Pre-conditioning is more like an art than science. Some choices include     \n",
    "    * Incomplete Cholesky. $\\mathbf{A} \\approx \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^T$, where $\\tilde{\\mathbf{L}}$ is a sparse approximate Cholesky factor. Then $\\tilde{\\mathbf{L}}^{-1} \\mathbf{A} \\tilde{\\mathbf{L}}^{-T} \\approx \\mathbf{I}$ (perfectly conditioned) and $\\mathbf{M} \\mathbf{y} = \\tilde{\\mathbf{L}} \\tilde {\\mathbf{L}}^T \\mathbf{y} = \\mathbf{r}$ is easy to solve.  \n",
    "    * Banded pre-conditioners.  \n",
    "    * Choose $\\mathbf{M}$ as a coarsened version of $\\mathbf{A}$.\n",
    "    * Subject knowledge. Knowledge about the structure and origin of a problem is often the key to devising efficient pre-conditioner. For example, see recent work of Stein, Chen, Anitescu (2012) for pre-conditioning large covariance matrices. http://epubs.siam.org/doi/abs/10.1137/110834469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of PCG\n",
    "\n",
    "[Preconditioners.jl](https://github.com/mohamed82008/Preconditioners.jl) wraps a bunch of preconditioners.\n",
    "\n",
    "We use the Wathen matrix (sparse and positive definite) as a test matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "include group.jl for user defined matrix generators\n",
      "verify download of index files...\n",
      "used remote site is https://sparse.tamu.edu/?per_page=All\n",
      "populating internal database...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30401×30401 SparseMatrixCSC{Float64,Int64} with 471601 stored entries:\n",
       "  [1    ,     1]  =  12.9515\n",
       "  [2    ,     1]  =  -12.9515\n",
       "  [3    ,     1]  =  4.31717\n",
       "  [202  ,     1]  =  -12.9515\n",
       "  [203  ,     1]  =  -17.2687\n",
       "  [303  ,     1]  =  4.31717\n",
       "  [304  ,     1]  =  -17.2687\n",
       "  [305  ,     1]  =  6.47576\n",
       "  [1    ,     2]  =  -12.9515\n",
       "  [2    ,     2]  =  69.0747\n",
       "  [3    ,     2]  =  -12.9515\n",
       "  [202  ,     2]  =  43.1717\n",
       "  ⋮\n",
       "  [30200, 30400]  =  36.1534\n",
       "  [30399, 30400]  =  -10.846\n",
       "  [30400, 30400]  =  57.8454\n",
       "  [30401, 30400]  =  -10.846\n",
       "  [30097, 30401]  =  5.42301\n",
       "  [30098, 30401]  =  -14.4614\n",
       "  [30099, 30401]  =  3.61534\n",
       "  [30199, 30401]  =  -14.4614\n",
       "  [30200, 30401]  =  -10.846\n",
       "  [30399, 30401]  =  3.61534\n",
       "  [30400, 30401]  =  -10.846\n",
       "  [30401, 30401]  =  10.846"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, MatrixDepot, IterativeSolvers, LinearAlgebra, SparseArrays\n",
    "\n",
    "# Wathen matrix of dimension 30401 x 30401\n",
    "A = matrixdepot(\"wathen\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m                      Sparsity Pattern\u001b[22m\n",
       "\u001b[90m         ┌──────────────────────────────────────────┐\u001b[39m    \n",
       "       \u001b[90m1\u001b[39m\u001b[90m │\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \u001b[31m> 0\u001b[39m\n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \u001b[34m< 0\u001b[39m\n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "        \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[35m⡀\u001b[39m\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "   \u001b[90m30401\u001b[39m\u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[35m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[90m│\u001b[39m    \n",
       "\u001b[90m         └──────────────────────────────────────────┘\u001b[39m    \n",
       "\u001b[90m         1\u001b[39m\u001b[90m                    \u001b[39m\u001b[90m                  30401\u001b[39m\n",
       "\u001b[0m                         nz = 471601"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using UnicodePlots\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005102687577359558"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparsity level\n",
    "count(!iszero, A) / length(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.16 MiB\n",
       "  allocs estimate:  23\n",
       "  --------------\n",
       "  minimum time:     143.481 ms (0.00% GC)\n",
       "  median time:      149.096 ms (0.00% GC)\n",
       "  mean time:        152.887 ms (0.20% GC)\n",
       "  maximum time:     175.385 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          33\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rhs\n",
    "b = ones(size(A, 1))\n",
    "# solve Ax=b by CG\n",
    "xcg = cg(A, b);\n",
    "@benchmark cg($A, $b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the incomplete cholesky preconditioner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22.499244 seconds (1.95 M allocations: 7.306 GiB, 1.02% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CholeskyPreconditioner{Float64,SparseMatrixCSC{Float64,Int64}}([3.59882 0.0 … 0.0 0.0; -3.59882 7.49154 … 0.0 0.0; … ; 0.0 0.0 … 5.09253 0.0; 0.0 0.0 … -0.16044 2.5317], 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Preconditioners\n",
    "@time p = CholeskyPreconditioner(A, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-conditioned conjugate gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.043872222791385e-7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solver Ax=b by PCG\n",
    "xpcg = cg(A, b, Pl=p)\n",
    "# same answer?\n",
    "norm(xcg - xpcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.16 MiB\n",
       "  allocs estimate:  32\n",
       "  --------------\n",
       "  minimum time:     13.694 ms (0.00% GC)\n",
       "  median time:      14.835 ms (0.00% GC)\n",
       "  mean time:        15.095 ms (0.71% GC)\n",
       "  maximum time:     18.935 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          331\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCG is >10 fold faster than CG\n",
    "@benchmark cg($A, $b, Pl=$p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Krylov subspace methods\n",
    "\n",
    "* We leant about CG/PCG, which is for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, $\\mathbf{A}$ pd.\n",
    "\n",
    "* **MINRES (minimum residual method)**: symmetric indefinite $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CG (bi-conjugate gradient)**: unsymmetric $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CGSTAB (Bi-CG stabilized)**: improved version of Bi-CG.\n",
    "\n",
    "* **GMRES (generalized minimum residual method)**: current _de facto_ method for unsymmetric $\\mathbf{A}$. E.g., PageRank problem.\n",
    "\n",
    "* **Lanczos method**: top eigen-pairs of a large symmetric matrix.\n",
    "\n",
    "* **Arnoldi method**: top eigen-pairs of a large unsymmetric matrix.\n",
    "\n",
    "* **Lanczos bidiagonalization** algorithm: top singular triplets of large matrix.\n",
    "\n",
    "* **LSQR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying CG to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I) \\beta = \\mathbf{X}^T \\mathbf{y}$.\n",
    "\n",
    "* **LSMR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying MINRES to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I) \\beta = \\mathbf{X}^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "### Matlab \n",
    "\n",
    "* Iterative methods for solving linear equations:  \n",
    "    `pcg`, `bicg`, `bicgstab`, `gmres`, ...\n",
    "* Iterative methods for top eigen-pairs and singular pairs:  \n",
    "    `eigs`, `svds`, ...\n",
    "* Pre-conditioner:  \n",
    "    `cholinc`, `luinc`, ...\n",
    "    \n",
    "* Get familiar with the **reverse communication interface (RCI)** for utilizing iterative solvers:\n",
    "```matlab\n",
    "x = gmres(A, b)\n",
    "x = gmres(@Afun, b)\n",
    "eigs(A)\n",
    "eigs(@Afun)\n",
    "```\n",
    "    \n",
    "### Julia\n",
    "\n",
    "* `eigs` and `svds` in the [Arpack.jl](https://github.com/JuliaLinearAlgebra/Arpack.jl) package. [Numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2019spring/slides/17-eigsvd/eigsvd.html#Lanczos/Arnoldi-iterative-method-for-top-eigen-pairs) later.\n",
    "\n",
    "* [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. [CG numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2019spring/slides/15-iterative/iterative.html#Numerical-examples)\n",
    "\n",
    "* Least squares examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squares example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.42 GiB\n",
       "  allocs estimate:  37941\n",
       "  --------------\n",
       "  minimum time:     5.619 s (1.37% GC)\n",
       "  median time:      5.619 s (1.37% GC)\n",
       "  mean time:        5.619 s (1.37% GC)\n",
       "  maximum time:     5.619 s (1.37% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, IterativeSolvers, LinearAlgebra, Random, SparseArrays\n",
    "\n",
    "Random.seed!(280) # seed\n",
    "n, p = 10000, 5000\n",
    "X = sprandn(n, p, 0.001) # iid standard normals with sparsity 0.01\n",
    "β = ones(p)\n",
    "y = X * β + randn(n)\n",
    "\n",
    "β̂_qr = X \\ y\n",
    "# least squares by QR\n",
    "@benchmark $X \\ $y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm(β̂_qr - β̂_lsqr) = 7.186597401024407e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  11.59 MiB\n",
       "  allocs estimate:  1346\n",
       "  --------------\n",
       "  minimum time:     26.817 ms (0.00% GC)\n",
       "  median time:      31.141 ms (11.82% GC)\n",
       "  mean time:        30.855 ms (6.77% GC)\n",
       "  maximum time:     36.993 ms (13.95% GC)\n",
       "  --------------\n",
       "  samples:          163\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂_lsqr = lsqr(X, y)\n",
    "@show norm(β̂_qr - β̂_lsqr)\n",
    "# least squares by lsqr\n",
    "@benchmark lsqr($X, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm(β̂_qr - β̂_lsmr) = 0.02228791821986182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  4.06 MiB\n",
       "  allocs estimate:  611\n",
       "  --------------\n",
       "  minimum time:     16.091 ms (0.00% GC)\n",
       "  median time:      17.828 ms (0.00% GC)\n",
       "  mean time:        18.311 ms (3.95% GC)\n",
       "  maximum time:     23.067 ms (19.72% GC)\n",
       "  --------------\n",
       "  samples:          273\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂_lsmr = lsmr(X, y)\n",
    "@show norm(β̂_qr - β̂_lsmr)\n",
    "# least squares by lsmr\n",
    "@benchmark lsmr($X, $y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Chapter 5 of [Numerical Optimization](https://ucla.worldcat.org/title/numerical-optimization/oclc/209918411&referer=brief_results) by Jorge Nocedal and Stephen Wright (1999).\n",
    "\n",
    "* Sections 11.3-11.5 of [Matrix Computations](https://ucla.worldcat.org/title/matrix-computations/oclc/824733531&referer=brief_results) by Gene Golub and Charles Van Loan (2013)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
