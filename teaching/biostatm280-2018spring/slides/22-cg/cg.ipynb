{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Conjugate-Gradient-and-Krylov-Subspace-Methods\" data-toc-modified-id=\"Conjugate-Gradient-and-Krylov-Subspace-Methods-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Conjugate Gradient and Krylov Subspace Methods</a></div><div class=\"lev2 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Conjugate-gradient-(CG)-method\" data-toc-modified-id=\"Conjugate-gradient-(CG)-method-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Conjugate gradient (CG) method</a></div><div class=\"lev2 toc-item\"><a href=\"#Pre-conditioned-conjugate-gradient-(PCG)\" data-toc-modified-id=\"Pre-conditioned-conjugate-gradient-(PCG)-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Pre-conditioned conjugate gradient (PCG)</a></div><div class=\"lev2 toc-item\"><a href=\"#Other-Krylov-subspace-methods\" data-toc-modified-id=\"Other-Krylov-subspace-methods-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Other Krylov subspace methods</a></div><div class=\"lev2 toc-item\"><a href=\"#Software\" data-toc-modified-id=\"Software-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Software</a></div><div class=\"lev3 toc-item\"><a href=\"#Matlab\" data-toc-modified-id=\"Matlab-151\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Matlab</a></div><div class=\"lev3 toc-item\"><a href=\"#Julia\" data-toc-modified-id=\"Julia-152\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Julia</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Krylov Subspace Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* Conjugate gradient is the top-notch iterative method for solving large, **structured** linear systems $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A}$ is pd.  \n",
    "Earlier we talked about Jacobi, Gauss-Seidel, and successive over-relaxation (SOR) as the classical iterative solvers. They are rarely used in practice due to slow convergence.  \n",
    "\n",
    "    [Kershaw's results](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub) for a fusion problem.\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky **conjugate gradient** | 25                   |\n",
    "\n",
    "\n",
    "* History: Hestenes (**UCLA** professor!) and Stiefel proposed conjugate gradient method in 1950s.\n",
    "\n",
    "Hestenes and Stiefel (1952), [Methods of conjugate gradients for solving linear systems](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf), _Jounral of Research of the National Bureau of Standards_.\n",
    "\n",
    "* Solve linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **pd**, is equivalent to \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\text{minimize} \\,\\, f(\\mathbf{x}) = \\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Denote $\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b} =: r(\\mathbf{x})$.\n",
    "\n",
    "## Conjugate gradient (CG) method\n",
    "\n",
    "* Consider a simple idea: coordinate descent, that is to update $x_j$ alternatingly. Same as the Gauss-Seidel iteration.\n",
    "\n",
    "<img src=\"coordinate_descent.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* A set of vectors $\\{\\mathbf{x}^{(0)},\\ldots,\\mathbf{x}^{(l)}\\}$ is said to be **conjugate with respect to $\\mathbf{A}$** if\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}_i^T \\mathbf{A} \\mathbf{p}_j = 0, \\quad \\text{for all } i \\ne j.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **Conjugate direction** method: Given a set of conjugate vectors $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(l)}\\}$, at iteration $t$, we search along the conjugate direction $\\mathbf{p}^{(t)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\alpha^{(t)} = - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "is the optimal step length.\n",
    "\n",
    "* Theorem: In conjugate direction method, $\\mathbf{x}^{(t)}$ converges to the solution in **at most** $n$ steps.\n",
    "\n",
    "    Intuition: Look at graph.\n",
    "    \n",
    "<img src=\"conjugate_direction.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* **Conjugate gradient** method. Idea: generate $\\mathbf{p}^{(t)}$ using only $\\mathbf{p}^{(t-1)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}^{(t)} = - \\mathbf{r}^{(t)} + \\beta^{(t)} \\mathbf{p}^{(t-1)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\beta^{(t)}$ is determined by the conjugacy condition $\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t)} = 0$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\beta^{(t)} = \\frac{\\mathbf{r}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}{\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **CG algorithm (preliminary version)**:  \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{A} \\mathbf{x}^{(t+1)} - \\mathbf{b}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{A} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "        \n",
    "* Theorem: With CG algorithm\n",
    "    0. $\\mathbf{r}^{(t)}$ are mutually orthogonal. \n",
    "    0. $\\{\\mathbf{r}^{(0)},\\ldots,\\mathbf{r}^{(t)}\\}$ is contained in the **Krylov subspace** of degree $t$ for $\\mathbf{r}^{(0)}$, denoted by\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        {\\cal K}(\\mathbf{r}^{(0)}; t) = \\text{span} \\{\\mathbf{r}^{(0)},\\mathbf{A} \\mathbf{r}^{(0)}, \\mathbf{A}^2 \\mathbf{r}^{(0)}, \\ldots, \\mathbf{A}^{t} \\mathbf{r}^{(0)}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    0. $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(t)}\\}$ is contained in ${\\cal K}(\\mathbf{r}^{(0)}; t)$. \n",
    "    0. $\\mathbf{p}^{(0)}, \\ldots, \\mathbf{p}^{(t)}$ are conjugate with respect to $\\mathbf{A}$.  \n",
    "The iterates $\\mathbf{x}^{(t)}$ converge to the solution in at most $n$ steps.\n",
    "\n",
    "* **CG algorithm (economical version)**: saves one matrix-vector multiplication.\n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{r}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "\n",
    "* Computation cost per iteration is one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t)}$.  \n",
    "Consider PageRank problem, $\\mathbf{A}$ has dimension $n \\approx 10^{10}$ but is highly structured (sparse + low rank). Each matrix vector multiplication takes $O(n)$.\n",
    "    \n",
    "* Theorem: If $\\mathbf{A}$ has $r$ distinct eigenvalues, $\\mathbf{x}^{(t)}$ converges to solution $\\mathbf{x}^*$ in at most $r$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-conditioned conjugate gradient (PCG)\n",
    "\n",
    "* Summary of conjugate gradient method for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ or equivalently minimizing $\\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} -  \\mathbf{b}^T \\mathbf{x}$:\n",
    "    * Each iteration needs one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t+1)}$. For structured $\\mathbf{A}$, often $O(n)$ cost per iteration.\n",
    "    * Guaranteed to converge in $n$ steps.\n",
    "    \n",
    "* Two important bounds for conjugate gradient algorithm:\n",
    "\n",
    "    Let $\\lambda_1 \\le \\cdots \\le \\lambda_n$ be the ordered eigenvalues of a pd $\\mathbf{A}$.  \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& \\left( \\frac{\\lambda_{n-t} - \\lambda_1}{\\lambda_{n-t} + \\lambda_1} \\right)^2 \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 \\\\\n",
    "    \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& 2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})}-1}{\\sqrt{\\kappa(\\mathbf{A})}+1} \\right)^{t} \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2,\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\kappa(\\mathbf{A}) = \\lambda_n/\\lambda_1$ is the condition number of $\\mathbf{A}$.\n",
    "\n",
    "<img src=\"cg_twocluster_spectrum.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src=\"cg_twocluster_iterates.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* Messages:\n",
    "    * Roughly speaking, if the eigenvalues of $\\mathbf{A}$ occur in $r$ distinct clusters, the CG iterates will _approximately_ solve the problem after $O(r)$ steps.  \n",
    "    * $\\mathbf{A}$ with a small condition number ($\\lambda_1 \\approx \\lambda_n$) converges fast.\n",
    "    \n",
    "* **Pre-conditioning**: Change of variables $\\widehat{\\mathbf{x}} = \\mathbf{C} \\mathbf{x}$ via a nonsingular $\\mathbf{C}$ and solve\n",
    "$$\n",
    "\t(\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}) \\widehat{\\mathbf{x}} = \\mathbf{C}^{-T} \\mathbf{b}.\n",
    "$$\n",
    "Choose $\\mathbf{C}$ such that \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has small condition number, or \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has clustered eigenvalues\n",
    "    * Inexpensive solution of $\\mathbf{C}^T \\mathbf{C} \\mathbf{y} = \\mathbf{r}$\n",
    "    \n",
    "* Preconditioned CG does not make use of $\\mathbf{C}$ explicitly, but rather the matrix $\\mathbf{M} = \\mathbf{C}^T \\mathbf{C}$.\n",
    "\n",
    "\n",
    "* **Preconditioned CG (PCG)** algorithm: \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$, pre-conditioner $\\mathbf{M}$\n",
    "    0. $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$\n",
    "    0. solve $\\mathbf{M} \\mathbf{y}^{(0)} = \\mathbf{r}^{(0)}$ for $\\mathbf{y}^{(0)}$\n",
    "    0. $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{y}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        0. Solve $\\mathbf{M} \\mathbf{y}^{(t+1)} \\gets \\mathbf{r}^{(t+1)}$ for $\\mathbf{y}^{(t+1)}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{y}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{y}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "\n",
    "    Remark: Only extra cost in the pre-conditioned CG algorithm is the need to solve the linear system $\\mathbf{M} \\mathbf{y} = \\mathbf{r}$.\n",
    "    \n",
    "* Pre-conditioning is more like an art than science. Some choices include     \n",
    "    * Incomplete Cholesky. $\\mathbf{A} \\approx \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^T$, where $\\tilde{\\mathbf{L}}$ is a sparse approximate Cholesky factor. Then $\\tilde{\\mathbf{L}}^{-1} \\mathbf{A} \\tilde{\\mathbf{L}}^{-T} \\approx \\mathbf{I}$ (perfectly conditioned) and $\\mathbf{M} \\mathbf{y} = \\tilde{\\mathbf{L}} \\tilde {\\mathbf{L}}^T \\mathbf{y} = \\mathbf{r}$ is easy to solve.  \n",
    "    * Banded pre-conditioners.  \n",
    "    * Choose $\\mathbf{M}$ as a coarsened version of $\\mathbf{A}$.\n",
    "    * Subject knowledge. Knowledge about the structure and origin of a problem is often the key to devising efficient pre-conditioner. For example, see recent work of Stein, Chen, Anitescu (2012) for pre-conditioning large covariance matrices. http://epubs.siam.org/doi/abs/10.1137/110834469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Krylov subspace methods\n",
    "\n",
    "* We leant about CG/PCG, which is for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, $\\mathbf{A}$ pd.\n",
    "\n",
    "* **MINRES (minimum residual method)**: symmetric indefinite $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CG (bi-conjugate gradient)**: unsymmetric $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CGSTAB (Bi-CG stabilized)**: improved version of Bi-CG.\n",
    "\n",
    "* **GMRES (generalized minimum residual method)**: current _de facto_ method for unsymmetric $\\mathbf{A}$. E.g., PageRank problem.\n",
    "\n",
    "* **Lanczos method**: top eigen-pairs of a large symmetric matrix.\n",
    "\n",
    "* **Arnoldi method**: top eigen-pairs of a large unsymmetric matrix.\n",
    "\n",
    "* **Lanczos bidiagonalization** algorithm: top singular triplets of large matrix.\n",
    "\n",
    "* **LSQR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying CG to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I)\\mathbf{x} = \\mathbf{X}^T \\mathbf{y}$.\n",
    "\n",
    "* **LSMR**: least square problem $\\min \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2$. Algebraically equivalent to applying MINRES to the normal equation $(\\mathbf{X}^T \\mathbf{X} + \\lambda^2 I)\\mathbf{x} = \\mathbf{X}^T \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "### Matlab \n",
    "\n",
    "* Iterative methods for solving linear equations:  \n",
    "    `pcg`, `bicg`, `bicgstab`, `gmres`, ...\n",
    "* Iterative methods for top eigen-pairs and singular pairs:  \n",
    "    `eigs`, `svds`, ...\n",
    "* Pre-conditioner:  \n",
    "    `cholinc`, `luinc`, ...\n",
    "    \n",
    "* Get familiar with the **reverse communication interface (RCI)** for utilizing iterative solvers:\n",
    "```matlab\n",
    "x = gmres(A, b)\n",
    "x = gmres(@Afun, b)\n",
    "eigs(A)}\n",
    "eigs(@Afun)\n",
    "```\n",
    "    \n",
    "### Julia\n",
    "\n",
    "* [`eigs`](https://docs.julialang.org/en/stable/stdlib/linalg/#Base.LinAlg.eigs-Tuple{Any}), [`svds`](https://docs.julialang.org/en/stable/stdlib/linalg/#Base.LinAlg.svds). [Numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/16-eigsvd/eigsvd.html#Lanczos/Arnoldi-iterative-method-for-top-eigen-pairs) \n",
    "\n",
    "* [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. [CG numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/14-iterative/iterative.html#Numerical-examples)\n",
    "\n",
    "* Least squares examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  5.17 MiB\n",
       "  allocs estimate:  98\n",
       "  --------------\n",
       "  minimum time:     41.517 ms (0.00% GC)\n",
       "  median time:      42.689 ms (0.00% GC)\n",
       "  mean time:        43.443 ms (1.19% GC)\n",
       "  maximum time:     88.582 ms (51.12% GC)\n",
       "  --------------\n",
       "  samples:          116\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, IterativeSolvers\n",
    "\n",
    "srand(280) # seed\n",
    "n, p = 1000, 500\n",
    "X = sprandn(n, p, 0.01)\n",
    "β = ones(p)\n",
    "y = X * β + randn(n)\n",
    "\n",
    "β̂_qr = X \\ y\n",
    "@benchmark X \\ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecnorm(β̂_qr - β̂_lsqr) = 1.0991691311790614e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  765.92 KiB\n",
       "  allocs estimate:  640\n",
       "  --------------\n",
       "  minimum time:     1.362 ms (0.00% GC)\n",
       "  median time:      1.413 ms (0.00% GC)\n",
       "  mean time:        1.528 ms (4.47% GC)\n",
       "  maximum time:     4.021 ms (63.27% GC)\n",
       "  --------------\n",
       "  samples:          3268\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂_lsqr = lsqr(X, y)\n",
    "@show vecnorm(β̂_qr - β̂_lsqr)\n",
    "@benchmark lsqr(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecnorm(β̂_qr - β̂_lsmr) = 1.0991691311790614e-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  286.19 KiB\n",
       "  allocs estimate:  336\n",
       "  --------------\n",
       "  minimum time:     876.849 μs (0.00% GC)\n",
       "  median time:      896.865 μs (0.00% GC)\n",
       "  mean time:        953.447 μs (2.76% GC)\n",
       "  maximum time:     4.396 ms (70.10% GC)\n",
       "  --------------\n",
       "  samples:          5234\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β̂_lsmr = lsqr(X, y)\n",
    "@show vecnorm(β̂_qr - β̂_lsmr)\n",
    "@benchmark lsmr(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.6.3\n",
      "Commit d55cadc350 (2018-05-28 20:20 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.9.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
