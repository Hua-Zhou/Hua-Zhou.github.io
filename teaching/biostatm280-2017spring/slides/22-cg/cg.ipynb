{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient and Krylov Subspace Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* (Linear) Conjugate gradient is the top-notch iterative method for solving large, **structured** linear systems $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A}$ is pd.  \n",
    "Earlier we talked about Jacobi, Gauss-Seidel, and successive over-relaxation (SOR) as the classical iterative solvers. They are rarely used in practice due to slow convergence.  \n",
    "\n",
    "    [Kershaw's results](http://www.sciencedirect.com/science/article/pii/0021999178900980?via%3Dihub) for a fusion problem.\n",
    "\n",
    "| Method                                 | Number of Iterations |\n",
    "|----------------------------------------|----------------------|\n",
    "| Gauss Seidel                           | 208,000              |\n",
    "| Block SOR methods                      | 765                  |\n",
    "| Incomplete Cholesky **conjugate gradient** | 25                   |\n",
    "\n",
    "\n",
    "* History: Hestenes (UCLA professor) and Stiefel proposed conjugate gradient method in 1950s.\n",
    "\n",
    "Hestenes and Stiefel (1952), [Methods of conjugate gradients for solving linear systems](http://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n6p409_A1b.pdf), _Jounral of Research of the National Bureau of Standards_.\n",
    "\n",
    "* Solve linear equation $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **pd**, is equivalent to \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\text{minimize} \\,\\, f(\\mathbf{x}) = \\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Denote $\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b} =: r(\\mathbf{x})$.\n",
    "\n",
    "## Conjugate gradient (CG) method\n",
    "\n",
    "* Consider a simple idea: coordinate descent, that is to update $x_j$ alternatingly. Same as the Gauss-Seidel iteration.\n",
    "\n",
    "<img src=\"coordinate_descent.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* A set of vectors $\\{\\mathbf{x}^{(0)},\\ldots,\\mathbf{x}^{(l)}\\}$ is said to be **conjugate with respect to $\\mathbf{A}$** if\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}_i^T \\mathbf{A} \\mathbf{p}_j = 0, \\quad \\text{for all } i \\ne j.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **Conjugate direction** method: Given a set of conjugate vectors $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(l)}\\}$, at iteration $t$, we search along the conjugate direction $\\mathbf{p}^{(t)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\alpha^{(t)} = - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "is the optimal step length.\n",
    "\n",
    "* Theorem: In conjugate direction method, $\\mathbf{x}^{(t)}$ converges to the solution in at most $n$ steps.\n",
    "\n",
    "    Intuition: Look at graph.\n",
    "    \n",
    "<img src=\"conjugate_direction.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* **Conjugate gradient** method. Idea: generate $\\mathbf{p}^{(t)}$ using only $\\mathbf{p}^{(t-1)}$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{p}^{(t)} = - \\mathbf{r}^{(t)} + \\beta^{(t)} \\mathbf{p}^{(t-1)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\beta^{(t)}$ is determined by the conjugacy condition $\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t)} = 0$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\beta^{(t)} = \\frac{\\mathbf{r}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}{\\mathbf{p}^{(t-1)T} \\mathbf{A} \\mathbf{p}^{(t-1)}}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* **CG algorithm (preliminary version)**:  \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets - \\frac{\\mathbf{r}^{(t)T} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{A} \\mathbf{x}^{(t+1)} - \\mathbf{b}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{A} \\mathbf{p}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "        \n",
    "* Theorem: With CG algorithm\n",
    "    0. $\\mathbf{r}^{(t)}$ are mutually orthogonal. \n",
    "    0. $\\{\\mathbf{r}^{(0)},\\ldots,\\mathbf{r}^{(t)}\\}$ is contained in the **Krylov subspace** of degree $t$ for $\\mathbf{r}^{(0)}$, denoted by\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        {\\cal K}(\\mathbf{r}^{(0)}; t) = \\text{span} \\{\\mathbf{r}^{(0)},\\mathbf{A} \\mathbf{r}^{(0)}, \\mathbf{A}^2 \\mathbf{r}^{(0)}, \\ldots, \\mathbf{A}^{t} \\mathbf{r}^{(0)}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    0. $\\{\\mathbf{p}^{(0)},\\ldots,\\mathbf{p}^{(t)}\\}$ is contained in ${\\cal K}(\\mathbf{r}^{(0)}; t)$. \n",
    "    0. $\\mathbf{p}^{(0)}, \\ldots, \\mathbf{p}^{(t)}$ are conjugate wrt $\\mathbf{A}$.  \n",
    "The iterates $\\mathbf{x}^{(t)}$ converge to the solution in at most $n$ steps.\n",
    "\n",
    "* **CG algorithm (economical version)**: saves one matrix-vector multiplication.\n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$\n",
    "    0. Initialize: $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$, $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{r}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{r}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "\n",
    "* Computation cost per iteration is one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t)}$.  \n",
    "Consider PageRank problem, $\\mathbf{A}$ has dimension $n \\approx 10^{10}$ but is highly structured (sparse + low rank). Each matrix vector multiplication takes $O(n)$.\n",
    "    \n",
    "* Theorem: If $\\mathbf{A}$ has $r$ distinct eigenvalues, $\\mathbf{x}^{(t)}$ converges to solution $\\mathbf{x}^*$ in at most $r$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-conditioned conjugate gradient (PCG)\n",
    "\n",
    "* Summary of conjugate gradient method for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ or equivalently minimizing $\\frac 12 \\mathbf{x}^T \\mathbf{A} \\mathbf{x} -  \\mathbf{b}^T \\mathbf{x}$:\n",
    "    * Each iteration needs one matrix vector multiplication: $\\mathbf{A} \\mathbf{p}^{(t+1)}$. For structured $\\mathbf{A}$, often $O(n)$ cost per iteration.\n",
    "    * Guaranteed to converge in $n$ steps.\n",
    "    \n",
    "* Two important bounds for conjugate gradient algorithm:\n",
    "\n",
    "    Let $\\lambda_1 \\le \\cdots \\le \\lambda_n$ be the ordered eigenvalues of a pd $\\mathbf{A}$.\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& \\left( \\frac{\\lambda_{n-t} - \\lambda_1}{\\lambda_{n-t} + \\lambda_1} \\right)^2 \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 \\\\\n",
    "        \\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2 &\\le& 2 \\left( \\frac{\\sqrt{\\kappa(\\mathbf{A})}-1}{\\sqrt{\\kappa(\\mathbf{A})}+1} \\right)^{t} \\|\\mathbf{x}^{(0)} - \\mathbf{x}^*\\|_{\\mathbf{A}}^2,\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    where $\\kappa(\\mathbf{A}) = \\lambda_n/\\lambda_1$ is the condition number of $\\mathbf{A}$.\n",
    "    \n",
    "<img src=\"cg_twocluster_spectrum.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src=\"cg_twocluster_iterates.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* Messages:\n",
    "    * Roughly speaking, if the eigenvalues of $\\mathbf{A}$ occur in $r$ distinct clusters, the CG iterates will _approximately_ solve the problem after $O(r)$ steps.  \n",
    "    * $\\mathbf{A}$ with a small condition number ($\\lambda_1 \\approx \\lambda_n$) converges fast.\n",
    "    \n",
    "* **Pre-conditioning**: Change of variables $\\widehat{\\mathbf{x}} = \\mathbf{C} \\mathbf{x}$ via a nonsingular $\\mathbf{C}$ and solve\n",
    "$$\n",
    "\t(\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}) \\widehat{\\mathbf{x}} = \\mathbf{C}^{-T} \\mathbf{b}.\n",
    "$$\n",
    "Choose $\\mathbf{C}$ such that \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has small condition number, or \n",
    "    * $\\mathbf{C}^{-T} \\mathbf{A} \\mathbf{C}^{-1}$ has clustered eigenvalues\n",
    "    * Inexpensive solution of $\\mathbf{C}^T \\mathbf{C} \\mathbf{y} = \\mathbf{r}$\n",
    "    \n",
    "* Preconditioned CG does not make use of $\\mathbf{C}$ explicitly, but rather the matrix $\\mathbf{M} = \\mathbf{C}^T \\mathbf{C}$.\n",
    "\n",
    "\n",
    "* **Preconditioned CG (PCG)** algorithm: \n",
    "\n",
    "    0. Given $\\mathbf{x}^{(0)}$, pre-conditioner $\\mathbf{M}$\n",
    "    0. $\\mathbf{r}^{(0)} \\gets \\mathbf{A} \\mathbf{x}^{(0)} - \\mathbf{b}$\n",
    "    0. solve $\\mathbf{M} \\mathbf{y}^{(0)} = \\mathbf{r}^{(0)}$ for $\\mathbf{y}^{(0)}$\n",
    "    0. $\\mathbf{p}^{(0)} \\gets - \\mathbf{r}^{(0)}$, $t=0$\n",
    "    0. While $\\mathbf{r}^{(0)} \\ne \\mathbf{0}$\n",
    "        0. $\\alpha^{(t)} \\gets \\frac{\\mathbf{r}^{(t)T} \\mathbf{y}^{(t)}}{\\mathbf{p}^{(t)T} \\mathbf{A} \\mathbf{p}^{(t)}}$\n",
    "        0. $\\mathbf{x}^{(t+1)} \\gets \\mathbf{x}^{(t)} + \\alpha^{(t)} \\mathbf{p}^{(t)}$\n",
    "        0. $\\mathbf{r}^{(t+1)} \\gets \\mathbf{r}^{(t)} + \\alpha^{(t)} \\mathbf{A} \\mathbf{p}^{(t)}$\n",
    "        0. Solve $\\mathbf{M} \\mathbf{y}^{(t+1)} \\gets \\mathbf{r}^{(t+1)}$ for $\\mathbf{y}^{(t+1)}$\n",
    "        0. $\\beta^{(t+1)} \\gets \\frac{\\mathbf{r}^{(t+1)T} \\mathbf{y}^{(t+1)}}{\\mathbf{r}^{(t)T} \\mathbf{r}^{(t)}}$\n",
    "        0. $\\mathbf{p}^{(t+1)} \\gets - \\mathbf{y}^{(t+1)} + \\beta^{(t+1)} \\mathbf{p}^{(t)}$\n",
    "        0. $t \\gets t+1$\n",
    "\n",
    "    Remark: Only extra cost in the pre-conditioned CG algorithm is the need to solve the linear system $\\mathbf{M} \\mathbf{y} = \\mathbf{r}$.\n",
    "    \n",
    "* Pre-conditioning is more like an art than science. Some choices include     \n",
    "    * Incomplete Cholesky. $\\mathbf{A} \\approx \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^T$, where $\\tilde{\\mathbf{L}}$ is a sparse approximate Cholesky factor. Then $\\tilde{\\mathbf{L}}^{-1} \\mathbf{A} \\tilde{\\mathbf{L}}^{-T} \\approx \\mathbf{I}$ (perfectly conditioned) and $\\mathbf{M} \\mathbf{y} = \\tilde{\\mathbf{L}} \\tilde {\\mathbf{L}}^T \\mathbf{y} = \\mathbf{r}$ is easy to solve.  \n",
    "    * Banded pre-conditioners.  \n",
    "    * Choose $\\mathbf{M}$ as a coarsened version of $\\mathbf{A}$.\n",
    "    * Subject knowledge. Knowledge about the structure and origin of a problem is often the key to devising efficient pre-conditioner. For example, see recent work of Stein, Chen, Anitescu (2012) for pre-conditioning large covariance matrices. http://epubs.siam.org/doi/abs/10.1137/110834469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Krylov subspace methods\n",
    "\n",
    "* We leant about CG/PCG, which is for solving $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, $\\mathbf{A}$ pd.\n",
    "\n",
    "* **MINRES (minimum residual method)**: symmetric indefinite $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CG (bi-conjugate gradient)**: unsymmetric $\\mathbf{A}$.\n",
    "\n",
    "* **Bi-CGSTAB (Bi-CG stabilized)**: improved version of Bi-CG.\n",
    "\n",
    "* **GMRES (generalized minimum residual method)**: current _de facto_ method for unsymmetric $\\mathbf{A}$. E.g., PageRank problem.\n",
    "\n",
    "* **Lanczos method**: top eigen-pairs of a large symmetric matrix.\n",
    "\n",
    "* **Arnoldi method**: top eigen-pairs of a large unsymmetric matrix.\n",
    "\n",
    "* **Lanczos bidiagonalization** algorithm: top singular triplets of large matrix.\n",
    "\n",
    "* LMSQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "### Matlab \n",
    "\n",
    "* Iterative methods for solving linear equations:  \n",
    "    `pcg`, `bicg`, `bicgstab`, `gmres`, ...\n",
    "* Iterative methods for top eigen-pairs and singular pairs:  \n",
    "    `eigs`, `svds`, ...\n",
    "* Pre-conditioner:  \n",
    "    `cholinc`, `luinc`, ...\n",
    "    \n",
    "* Get familiar with the **reverse communication interface (RCI)** for utilizing iterative solvers:\n",
    "```matlab\n",
    "x = gmres(A, b)\n",
    "x = gmres(@Afun, b)\n",
    "eigs(A)}\n",
    "eigs(@Afun)\n",
    "```\n",
    "    \n",
    "### Julia\n",
    "\n",
    "* [`eigs`](https://docs.julialang.org/en/stable/stdlib/linalg/#Base.eigs), [`svds`](https://docs.julialang.org/en/stable/stdlib/linalg/#Base.svds). [Numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/16-eigsvd/eigsvd.html#Lanczos/Arnoldi-iterative-method-for-top-eigen-pairs) \n",
    "\n",
    "* [`IterativeSolvers.jl`](https://github.com/JuliaLang/IterativeSolvers.jl/issues/1) package. [Numerical examples](http://hua-zhou.github.io/teaching/biostatm280-2017spring/slides/14-iterative/iterative.html#Numerical-examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
